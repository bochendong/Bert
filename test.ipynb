{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongpochen/opt/anaconda3/envs/torch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import random\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, corpus_path = './data/eng-fra.txt', vocab = None , seq_len = 20, corpus_lines=None):\n",
    "        self.vocab = vocab\n",
    "        self.seq_len = seq_len\n",
    "        self.corpus_lines = 0\n",
    "        self.corpus_path = corpus_path\n",
    "        self.lines = []\n",
    "\n",
    "        # Reopen the file to read the lines\n",
    "        with open('./data/eng-fra.txt', \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in tqdm.tqdm(f, desc=\"Loading Dataset\", total=corpus_lines):\n",
    "                self.lines.append(line.strip())\n",
    "\n",
    "        self.corpus_lines = len(self.lines)\n",
    "\n",
    "    def get_corpus_line(self, item):\n",
    "        return self.lines[item][0], self.lines[item][1]\n",
    "    \n",
    "    def get_random_line(self):\n",
    "        return self.lines[random.randrange(self.corpus_lines)][1]\n",
    "    \n",
    "    def random_sent(self, index):\n",
    "        t1, t2 = self.get_corpus_line(index)\n",
    "\n",
    "        # output_text, label(isNotNext:0, isNext:1)\n",
    "        if random.random() > 0.5:\n",
    "            return t1, t2, 1\n",
    "        else:\n",
    "            return t1, self.get_random_line(), 0\n",
    "        \n",
    "    def random_word(self, sentence):\n",
    "        tokens = sentence.split()\n",
    "        output_label = []\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            prob = random.random()\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "\n",
    "                # 80% randomly change token to mask token\n",
    "                if prob < 0.8:\n",
    "                    tokens[i] = self.vocab.mask_index\n",
    "\n",
    "                # 10% randomly change token to random token\n",
    "                elif prob < 0.9:\n",
    "                    tokens[i] = random.randrange(len(self.vocab))\n",
    "\n",
    "                # 10% randomly change token to current token\n",
    "                else:\n",
    "                    tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)\n",
    "\n",
    "                output_label.append(self.vocab.stoi.get(token, self.vocab.unk_index))\n",
    "\n",
    "            else:\n",
    "                tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)\n",
    "                output_label.append(0)\n",
    "\n",
    "        return tokens, output_label\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        t1, t2, is_next_label = self.random_sent(item)\n",
    "        t1_random, t1_label = self.random_word(t1)\n",
    "        t2_random, t2_label = self.random_word(t2)\n",
    "\n",
    "        # [CLS] tag = SOS tag, [SEP] tag = EOS tag\n",
    "        t1 = [self.vocab.sos_index] + t1_random + [self.vocab.eos_index]\n",
    "        t2 = t2_random + [self.vocab.eos_index]\n",
    "\n",
    "        t1_label = [self.vocab.pad_index] + t1_label + [self.vocab.pad_index]\n",
    "        t2_label = t2_label + [self.vocab.pad_index]\n",
    "\n",
    "        segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n",
    "        bert_input = (t1 + t2)[:self.seq_len]\n",
    "        bert_label = (t1_label + t2_label)[:self.seq_len]\n",
    "\n",
    "        padding = [self.vocab.pad_index for _ in range(self.seq_len - len(bert_input))]\n",
    "        bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n",
    "\n",
    "        output = {\"bert_input\": bert_input,\n",
    "                  \"bert_label\": bert_label,\n",
    "                  \"segment_label\": segment_label,\n",
    "                  \"is_next\": is_next_label}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.corpus_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset: 100%|██████████| 135842/135842 [00:00<00:00, 1818306.30it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_lines = 0\n",
    "lines = []\n",
    "\n",
    "with open('./data/eng-fra.txt', \"r\", encoding=\"utf-8\") as f:\n",
    "    # Counting lines for tqdm progress\n",
    "    for _ in f:\n",
    "        corpus_lines += 1\n",
    "\n",
    "# Reopen the file to read the lines\n",
    "with open('./data/eng-fra.txt', \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm.tqdm(f, desc=\"Loading Dataset\", total=corpus_lines):\n",
    "        lines.append(line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135842"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " train_dataset = BERTDataset(args.train_dataset, vocab, seq_len=args.seq_len,\n",
    "                                corpus_lines=args.corpus_lines, on_memory=args.on_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset: 135842it [00:00, 153362.44it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "with open('./data/eng-fra.txt', \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm.tqdm(f, desc=\"Loading Dataset\"):\n",
    "        if isinstance(line, list):\n",
    "            words = line\n",
    "        else:\n",
    "            words = line.replace(\"\\n\", \"\").replace(\"\\t\", \" \").split()\n",
    "\n",
    "        for word in words:\n",
    "            counter[word] += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
